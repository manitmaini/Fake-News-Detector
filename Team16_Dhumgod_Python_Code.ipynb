{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Team 16 - Dhumgod\n",
        "Team Members:\n",
        "   - Aditya Narasimhan Sampath - CB.EN.U4ELC19004\n",
        "   - Manit Maini - CB.EN.U4ELC19029\n",
        "   - Pradhumna Guruprasad - CB.EN.U4ELC19036\n",
        "   - Shreyas Nagesh - CB.EN.U4ELC19052"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the Neccessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzVJYggQ_Hzm",
        "outputId": "f61d5866-f536-4115-99c2-b5eb56a3f474"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger') \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reading the Training Data and Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NS1bbMO_Hzt",
        "outputId": "eac4f75c-4de8-43a9-8f86-b4200731f288"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>At a time when the perfect outfit is just one ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  Get the latest from TODAY Sign up for our news...     1\n",
              "1  2d  Conan On The Funeral Trump Will Be Invited...     1\n",
              "2  It’s safe to say that Instagram Stories has fa...     0\n",
              "3  Much like a certain Amazon goddess with a lass...     0\n",
              "4  At a time when the perfect outfit is just one ...     0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainData = pd.read_csv(\"train.csv\", sep='\\t', encoding='utf-8')\n",
        "testData = pd.read_csv(\"test.csv\", sep='\\t', encoding='utf-8')\n",
        "trainData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1615\n"
          ]
        }
      ],
      "source": [
        "for ind in trainData.index:\n",
        "    if trainData['label'][ind] == 'label':\n",
        "        print(ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainData = trainData.drop(1615)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0cPzTN2d_Hzx",
        "outputId": "b3f9b4a7-eeed-4290-c342-11e59b0188c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>The 2017 Teen Choice Awards ceremony was held ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>The concert, part of “The Joshua Tree Tour,” w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Selena Gomez refuses to talk to her mother abo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>This is worse than a lump of coal in your stoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>Luann De Lesseps is going to rehab after her a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                               text\n",
              "0   2  The 2017 Teen Choice Awards ceremony was held ...\n",
              "1   3  The concert, part of “The Joshua Tree Tour,” w...\n",
              "2   4  Selena Gomez refuses to talk to her mother abo...\n",
              "3   5  This is worse than a lump of coal in your stoc...\n",
              "4   6  Luann De Lesseps is going to rehab after her a..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testData.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merging the Training Data and Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RsQTdttk_Hzz"
      },
      "outputs": [],
      "source": [
        "mergedData = pd.concat((trainData.drop(['label'], axis=1), testData.drop('id', axis=1)), axis=0).reset_index().drop('index', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5YEFp2LL_Hz0",
        "outputId": "87ff3c2b-e30a-4ebc-a85c-875361a57ecc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>At a time when the perfect outfit is just one ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Get the latest from TODAY Sign up for our news...\n",
              "1  2d  Conan On The Funeral Trump Will Be Invited...\n",
              "2  It’s safe to say that Instagram Stories has fa...\n",
              "3  Much like a certain Amazon goddess with a lass...\n",
              "4  At a time when the perfect outfit is just one ..."
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mergedData.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenizing and Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OEQ8_X6f_Hz1"
      },
      "outputs": [],
      "source": [
        "def tokenization(data):\n",
        "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
        "    tokenData = []\n",
        "    for values in data.text:\n",
        "        tokenData.append(tokenizer.tokenize(values))\n",
        "    return tokenData\n",
        "    \n",
        "def clean_stopwords(tokendata):\n",
        "    sw = stopwords.words('english')\n",
        "    clean_data = [] \n",
        "    for data in tokendata:\n",
        "        clean_text = [words.lower() for words in data if words.lower() not in sw]\n",
        "        clean_data.append(clean_text) \n",
        "    return clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nis16Ofe_Hz2",
        "outputId": "572908fc-5383-4cd0-949c-7cd19e04bf6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "326 193\n"
          ]
        }
      ],
      "source": [
        "f = open('tokenized_data.txt', 'a', encoding=\"utf-8\")\n",
        "tokenData = tokenization(mergedData)\n",
        "for i in tokenData:\n",
        "    f.write(f'{i}\\n')\n",
        "f.close()\n",
        "\n",
        "f = open('clean_data.txt', 'a', encoding=\"utf-8\")\n",
        "clean_data = clean_stopwords(tokenData)\n",
        "for i in clean_data:\n",
        "    f.write(f'{i}\\n')\n",
        "f.close()\n",
        "\n",
        "print(len(tokenData[0]), len(clean_data[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lemmatizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHUuRZkz_Hz8",
        "outputId": "5fa1c8cc-7aca-4077-821c-0b0bc4c9a69e"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(sentence)  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    f = open('WordTag.txt', 'a', encoding=\"utf-8\")\n",
        "    for word, tag in wordnet_tagged:\n",
        "        f.write(f'({word}, {tag})\\n')\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    f.close()\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "r = []\n",
        "fi = open('Lemma_Sent.txt', 'a', encoding=\"utf-8\")\n",
        "for i in range(len(clean_data)):\n",
        "    temp = lemmatize_sentence(clean_data[i])\n",
        "    r.append(temp)\n",
        "    fi.write(f'{temp}\\n')\n",
        "\n",
        "fi.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(r, columns=['text'])\n",
        "df.to_csv('Lemma_Sent.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data = r[:len(trainData)]\n",
        "test_data = r[len(trainData):]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, trainData.label, test_size=0.2, random_state = 42)\n",
        "y_train = np.array(y_train.to_frame()['label'].astype(int).tolist())\n",
        "y_test = np.array(y_test.to_frame()['label'].astype(int).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(X_train, columns=['text'])\n",
        "df.to_csv('Lemma_Sent.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3988 174\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(len(X_train_seq), len(X_train_seq[0]))\n",
        "\n",
        "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=1000)\n",
        "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(998, 1000) (3988, 1000)\n"
          ]
        }
      ],
      "source": [
        "print(X_test_seq_trunc.shape, X_train_seq_trunc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,  4570,   133, 18268],\n",
              "       [29238,     8,    14, ...,  2293,     9,  3055],\n",
              "       [    0,     0,     0, ...,  4853,  1689,  1730],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,    89,   609,   624],\n",
              "       [ 1360,   542,     3, ...,  1838, 15179,   111],\n",
              "       [    0,     0,     0, ...,  3678,   138,   519]])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_seq_trunc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocabSize = len(tokenizer.word_index) + 1\n",
        "\n",
        "cnnModel = Sequential()\n",
        "cnnModel.add(layers.Embedding(vocabSize, 16, input_length=1000))\n",
        "cnnModel.add(layers.Dropout(0.2))\n",
        "\n",
        "cnnModel.add(layers.Convolution1D(32,4,activation='relu'))\n",
        "cnnModel.add(layers.Dropout(0.3))\n",
        "\n",
        "cnnModel.add(layers.AveragePooling1D())\n",
        "\n",
        "cnnModel.add(layers.Convolution1D(64,4,activation='relu'))\n",
        "cnnModel.add(layers.Dropout(0.4))\n",
        "\n",
        "cnnModel.add(layers.AveragePooling1D())\n",
        "\n",
        "cnnModel.add(layers.Flatten())\n",
        "cnnModel.add(layers.Dropout(0.6))\n",
        "\n",
        "cnnModel.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "#cnnModel.add(layers.Dense(units=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1000, 16)          1141040   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1000, 16)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 997, 32)           2080      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 997, 32)           0         \n",
            "                                                                 \n",
            " average_pooling1d_2 (Averag  (None, 498, 32)          0         \n",
            " ePooling1D)                                                     \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 495, 64)           8256      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 495, 64)           0         \n",
            "                                                                 \n",
            " average_pooling1d_3 (Averag  (None, 247, 64)          0         \n",
            " ePooling1D)                                                     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 15808)             0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 15808)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 15809     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,167,185\n",
            "Trainable params: 1,167,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cnnModel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6746 - binary_accuracy: 0.5980\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.58216, saving model to model.h5\n",
            "16/16 [==============================] - 10s 560ms/step - loss: 0.6746 - binary_accuracy: 0.5980 - val_loss: 0.6790 - val_binary_accuracy: 0.5822\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6721 - binary_accuracy: 0.5995\n",
            "Epoch 2: val_binary_accuracy did not improve from 0.58216\n",
            "16/16 [==============================] - 8s 515ms/step - loss: 0.6721 - binary_accuracy: 0.5995 - val_loss: 0.6784 - val_binary_accuracy: 0.5822\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6701 - binary_accuracy: 0.5995\n",
            "Epoch 3: val_binary_accuracy did not improve from 0.58216\n",
            "16/16 [==============================] - 8s 491ms/step - loss: 0.6701 - binary_accuracy: 0.5995 - val_loss: 0.6762 - val_binary_accuracy: 0.5822\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6640 - binary_accuracy: 0.5995\n",
            "Epoch 4: val_binary_accuracy did not improve from 0.58216\n",
            "16/16 [==============================] - 8s 493ms/step - loss: 0.6640 - binary_accuracy: 0.5995 - val_loss: 0.6686 - val_binary_accuracy: 0.5822\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6367 - binary_accuracy: 0.6256\n",
            "Epoch 5: val_binary_accuracy improved from 0.58216 to 0.64429, saving model to model.h5\n",
            "16/16 [==============================] - 8s 506ms/step - loss: 0.6367 - binary_accuracy: 0.6256 - val_loss: 0.6228 - val_binary_accuracy: 0.6443\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4997 - binary_accuracy: 0.7462\n",
            "Epoch 6: val_binary_accuracy improved from 0.64429 to 0.74749, saving model to model.h5\n",
            "16/16 [==============================] - 8s 528ms/step - loss: 0.4997 - binary_accuracy: 0.7462 - val_loss: 0.5220 - val_binary_accuracy: 0.7475\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2801 - binary_accuracy: 0.8867\n",
            "Epoch 7: val_binary_accuracy improved from 0.74749 to 0.76152, saving model to model.h5\n",
            "16/16 [==============================] - 8s 505ms/step - loss: 0.2801 - binary_accuracy: 0.8867 - val_loss: 0.5739 - val_binary_accuracy: 0.7615\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1560 - binary_accuracy: 0.9443\n",
            "Epoch 8: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.1560 - binary_accuracy: 0.9443 - val_loss: 0.6496 - val_binary_accuracy: 0.7525\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0894 - binary_accuracy: 0.9702\n",
            "Epoch 9: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 515ms/step - loss: 0.0894 - binary_accuracy: 0.9702 - val_loss: 0.7359 - val_binary_accuracy: 0.7485\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0674 - binary_accuracy: 0.9764\n",
            "Epoch 10: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 510ms/step - loss: 0.0674 - binary_accuracy: 0.9764 - val_loss: 0.8195 - val_binary_accuracy: 0.7365\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0563 - binary_accuracy: 0.9799\n",
            "Epoch 11: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.0563 - binary_accuracy: 0.9799 - val_loss: 0.9135 - val_binary_accuracy: 0.7455\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0479 - binary_accuracy: 0.9829\n",
            "Epoch 12: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0479 - binary_accuracy: 0.9829 - val_loss: 0.9625 - val_binary_accuracy: 0.7435\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0426 - binary_accuracy: 0.9852\n",
            "Epoch 13: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.0426 - binary_accuracy: 0.9852 - val_loss: 0.9601 - val_binary_accuracy: 0.7335\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0389 - binary_accuracy: 0.9867\n",
            "Epoch 14: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 497ms/step - loss: 0.0389 - binary_accuracy: 0.9867 - val_loss: 1.0029 - val_binary_accuracy: 0.7415\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0358 - binary_accuracy: 0.9867\n",
            "Epoch 15: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0358 - binary_accuracy: 0.9867 - val_loss: 1.0747 - val_binary_accuracy: 0.7405\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0341 - binary_accuracy: 0.9892\n",
            "Epoch 16: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 497ms/step - loss: 0.0341 - binary_accuracy: 0.9892 - val_loss: 0.9952 - val_binary_accuracy: 0.7325\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0300 - binary_accuracy: 0.9912\n",
            "Epoch 17: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.0300 - binary_accuracy: 0.9912 - val_loss: 1.0388 - val_binary_accuracy: 0.7224\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0279 - binary_accuracy: 0.9902\n",
            "Epoch 18: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 511ms/step - loss: 0.0279 - binary_accuracy: 0.9902 - val_loss: 1.0686 - val_binary_accuracy: 0.7295\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0247 - binary_accuracy: 0.9912\n",
            "Epoch 19: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 9s 534ms/step - loss: 0.0247 - binary_accuracy: 0.9912 - val_loss: 1.1534 - val_binary_accuracy: 0.7275\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0284 - binary_accuracy: 0.9897\n",
            "Epoch 20: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 9s 541ms/step - loss: 0.0284 - binary_accuracy: 0.9897 - val_loss: 1.1181 - val_binary_accuracy: 0.7325\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0258 - binary_accuracy: 0.9882\n",
            "Epoch 21: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0258 - binary_accuracy: 0.9882 - val_loss: 1.0721 - val_binary_accuracy: 0.7315\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0281 - binary_accuracy: 0.9905\n",
            "Epoch 22: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0281 - binary_accuracy: 0.9905 - val_loss: 1.2232 - val_binary_accuracy: 0.7315\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0267 - binary_accuracy: 0.9890\n",
            "Epoch 23: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 496ms/step - loss: 0.0267 - binary_accuracy: 0.9890 - val_loss: 1.0982 - val_binary_accuracy: 0.7385\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0264 - binary_accuracy: 0.9915\n",
            "Epoch 24: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 517ms/step - loss: 0.0264 - binary_accuracy: 0.9915 - val_loss: 1.1244 - val_binary_accuracy: 0.7285\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0240 - binary_accuracy: 0.9922\n",
            "Epoch 25: val_binary_accuracy did not improve from 0.76152\n",
            "16/16 [==============================] - 8s 497ms/step - loss: 0.0240 - binary_accuracy: 0.9922 - val_loss: 1.1387 - val_binary_accuracy: 0.7234\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x171e67b4670>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnnModel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "checkpoint = ModelCheckpoint('model.h5',verbose=1, monitor='val_binary_accuracy',save_best_only=True, mode='auto')\n",
        "\n",
        "#checkpoint = ModelCheckpoint('model.h5',verbose=1, monitor='loss',save_best_only=True, mode='auto')\n",
        "\n",
        "\n",
        "cnnModel.fit(x=X_train_seq_trunc, y=y_train, batch_size=256, epochs=25, \n",
        "                validation_data=(X_test_seq_trunc,y_test), callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnnModel = keras.models.load_model('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 9ms/step - loss: 0.5739 - binary_accuracy: 0.7615\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.5738862156867981, 0.7615230679512024]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = cnnModel.evaluate(X_test_seq_trunc, y_test)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "pred = cnnModel.predict(X_test_seq_trunc).ravel()\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc\n",
        "auc_keras = auc(fpr_keras, tpr_keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_keras, tpr_keras, label='CNN Classifier AUC = {:.3f})'.format(auc_keras))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "News: It’s safe to say that Instagram Stories has far surpassed its competitor Snapchat in popularity since it’s inception two years ago—and your favorite celebrities have hopped on the social media trend. Unlike a highly curated photo feed, Instagram Stories is where celebrities seem to be comfortable enough to be raw and open.  Need something to do while you’re waiting in line or on a short break? Take a peek at these celebrities’ Instagram Stories for some surprisingly engaging entertainment.  Busy Philipps, @busyphilipps  A fantastic story teller, Busy was dubbed by The New Yorker as “the breakout star of Instagram Stories”. She captures everything from morning workouts to paparazzi run-ins and everything in between. If it isn’t on Busy’s story, I am assuming it didn’t happen.  Mandy Moore, @mandymooremm  Following Mandy Moore for her many This is Us behind-the-scenes stories is worth it alone! She also InstaStoried her home being built and decorated, her Mount Kilimanjaro climb, and the preparation behind all the Hollywood red carpet events she’s recently attended.  Chrissy Teigen, @chrissyteigen  Because if you follow and love her on Twitter and Snapchat, why wouldn’t you watch her Instagram Stories for more of her humor, cooking, and adorable daughter Luna?  Reese Witherspoon, @reesewitherspoon  Reese may be one of the biggest stars in the world, but she is a down-to-earth breath of fresh air on her Instagram stories!  Sarah Hyland, @sarahhyland  Somehow the Modern Family star makes eating dinner solo while watching The Bachelor interesting enough to keep watching.  Candace Cameron Bure, @candacecbure  I’ve had a soft spot for Candace since growing up with her on Full House, and I am living for the resurgence of her career! Follow Candace for Instagram Stories about fashion, family, workouts, and the behind-the-scenes of her on movie and TV sets.  Eva Chen, @evachen212  Though she may be biased about using the platform since becoming Director of Fashion Partnerships at Instagram, she’s still one to watch! Eva chronicles her daily life from early mornings with her adorable kids to international fashion week events.  Jessica Alba, @jessicaalba  This new mama of three adorably shows how she juggles her home and work life. And now that she is preparing to be back on TV, there’s sure to be some behind-the-scenes sneak peeks!  Kristin Cavallari, @kristincavallari  From reality TV to cookbook author and fashion designer, Kristin shares her own recipes and has just opened her first brick-and-mortar  for her fashion and home design line, Uncommon James.  Spencer Pratt, @spencerpratt  Hear me out! The Hills alum may have a villainous bad rap from his 15 minutes of fame, but his Instagram stories highlight his  family life with his 6-month-old son Gunner and wife Heidi. His many obsessions include hummingbirds, working out, coffee, and Taylor Swift., Label: 0\n"
          ]
        }
      ],
      "source": [
        "for i in trainData.index:\n",
        "    if trainData['label'][i] == '0':\n",
        "        print(f\"News: {trainData['text'][i]}, Label: {trainData['label'][i]}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "News: Ryan Reynolds and Blake Lively routinely troll each other on social media, but the Deadpool star took it to the next level when he was forced to defend his marriage this weekend.  An \"insider\" allegedly told IB Times, \"The past few years, Ryan's been working out of town while Blake stays home with the kids. The distance between them has taken its toll.\" The gossip site tweeted, \"Ryan Reynolds and wife Blake Lively struggling to spend 'quality time'\" together.  Advertisement - Continue Reading Below  And Reynolds' response to this report? \"I wish. I could use a little 'me time'.\"  As per usual, Reynolds is ready to tease his wife on social media, which suggests that their marriage is more than fine.  I wish. I could use a little “me time”. https://t.co/S6kXFsWaMe — Ryan Reynolds (@VancityReynolds) March 31, 2018  Reynolds and Lively have a long history of trolling one another, and their constant joking appears to help keep their relationship alive.  But, perhaps the couple's sweetest online moment, was when Reynolds was photographed with Lively for the Humans of New York Instagram account. Explaining why their marriage worked, Reynolds revealed:  \"She always responds with empathy. She meets anger with empathy. She meets hate with empathy. She'll take the time to imagine what happened to a person when they were five or six years old. And she's made me a more empathetic person. I had a very fractured relationship with my father. Before he died, she made me remember things I didn't want to remember. She made me remember the good times.\"  It's pretty clear that Reynolds and Lively are just fine right now, thank you very much., Label: 1\n"
          ]
        }
      ],
      "source": [
        "for i in trainData.index:\n",
        "    if i == 1511:\n",
        "        print(f\"News: {trainData['text'][i]}, Label: {trainData['label'][i]}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "cnnModel = Sequential()\n",
        "cnnModel.add(layers.Embedding(vocabSize, 16, input_length=1000))\n",
        "cnnModel.add(layers.Dropout(0.2))\n",
        "\n",
        "cnnModel.add(layers.Convolution1D(32,4,activation='relu'))\n",
        "cnnModel.add(layers.Dropout(0.3))\n",
        "\n",
        "cnnModel.add(layers.AveragePooling1D())\n",
        "\n",
        "cnnModel.add(layers.Convolution1D(64,4,activation='relu'))\n",
        "cnnModel.add(layers.Dropout(0.4))\n",
        "\n",
        "cnnModel.add(layers.AveragePooling1D())\n",
        "\n",
        "cnnModel.add(layers.Flatten())\n",
        "cnnModel.add(layers.Dropout(0.6))\n",
        "\n",
        "cnnModel.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "cnnModel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "normal = MinMaxScaler().fit(X_train_seq_trunc)\n",
        "X_train_seq_trunc_n = normal.transform(X_train_seq_trunc)\n",
        "X_test_seq_trunc_n = normal.transform(X_test_seq_trunc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(998, 1000) (3988, 1000)\n"
          ]
        }
      ],
      "source": [
        "print(X_test_seq_trunc_n.shape, X_train_seq_trunc_n.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        6.42575928e-02, 1.89202646e-03, 2.57484777e-01],\n",
              "       [4.39187058e-01, 1.16900956e-04, 1.98767641e-04, ...,\n",
              "        3.22412823e-02, 1.28031866e-04, 4.30480379e-02],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        6.82367829e-02, 2.40273135e-02, 2.43713351e-02],\n",
              "       ...,\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        1.25140607e-03, 8.66348958e-03, 8.78157420e-03],\n",
              "       [2.04287023e-02, 7.92003975e-03, 4.25930658e-05, ...,\n",
              "        2.58436445e-02, 2.15932854e-01, 1.55051872e-03],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        5.17154106e-02, 1.96315527e-03, 7.30153360e-03]])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_seq_trunc_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6764 - binary_accuracy: 0.5980\n",
            "Epoch 1: loss improved from inf to 0.67639, saving model to model1.h5\n",
            "16/16 [==============================] - 11s 582ms/step - loss: 0.6764 - binary_accuracy: 0.5980 - val_loss: 0.6796 - val_binary_accuracy: 0.5822\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6734 - binary_accuracy: 0.5995\n",
            "Epoch 2: loss improved from 0.67639 to 0.67339, saving model to model1.h5\n",
            "16/16 [==============================] - 9s 553ms/step - loss: 0.6734 - binary_accuracy: 0.5995 - val_loss: 0.6810 - val_binary_accuracy: 0.5822\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6751 - binary_accuracy: 0.5995\n",
            "Epoch 3: loss did not improve from 0.67339\n",
            "16/16 [==============================] - 8s 524ms/step - loss: 0.6751 - binary_accuracy: 0.5995 - val_loss: 0.6810 - val_binary_accuracy: 0.5822\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6728 - binary_accuracy: 0.5995\n",
            "Epoch 4: loss improved from 0.67339 to 0.67280, saving model to model1.h5\n",
            "16/16 [==============================] - 8s 507ms/step - loss: 0.6728 - binary_accuracy: 0.5995 - val_loss: 0.6804 - val_binary_accuracy: 0.5822\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6746 - binary_accuracy: 0.5995\n",
            "Epoch 5: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 491ms/step - loss: 0.6746 - binary_accuracy: 0.5995 - val_loss: 0.6800 - val_binary_accuracy: 0.5822\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6739 - binary_accuracy: 0.5995\n",
            "Epoch 6: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 507ms/step - loss: 0.6739 - binary_accuracy: 0.5995 - val_loss: 0.6813 - val_binary_accuracy: 0.5822\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6740 - binary_accuracy: 0.5995\n",
            "Epoch 7: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 508ms/step - loss: 0.6740 - binary_accuracy: 0.5995 - val_loss: 0.6802 - val_binary_accuracy: 0.5822\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6740 - binary_accuracy: 0.5995\n",
            "Epoch 8: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 490ms/step - loss: 0.6740 - binary_accuracy: 0.5995 - val_loss: 0.6806 - val_binary_accuracy: 0.5822\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6736 - binary_accuracy: 0.5995\n",
            "Epoch 9: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 512ms/step - loss: 0.6736 - binary_accuracy: 0.5995 - val_loss: 0.6798 - val_binary_accuracy: 0.5822\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6737 - binary_accuracy: 0.5995\n",
            "Epoch 10: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 510ms/step - loss: 0.6737 - binary_accuracy: 0.5995 - val_loss: 0.6829 - val_binary_accuracy: 0.5822\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6729 - binary_accuracy: 0.5995\n",
            "Epoch 11: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 482ms/step - loss: 0.6729 - binary_accuracy: 0.5995 - val_loss: 0.6798 - val_binary_accuracy: 0.5822\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6739 - binary_accuracy: 0.5995\n",
            "Epoch 12: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 476ms/step - loss: 0.6739 - binary_accuracy: 0.5995 - val_loss: 0.6803 - val_binary_accuracy: 0.5822\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6733 - binary_accuracy: 0.5995\n",
            "Epoch 13: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 488ms/step - loss: 0.6733 - binary_accuracy: 0.5995 - val_loss: 0.6796 - val_binary_accuracy: 0.5822\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6744 - binary_accuracy: 0.5995\n",
            "Epoch 14: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.6744 - binary_accuracy: 0.5995 - val_loss: 0.6796 - val_binary_accuracy: 0.5822\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6740 - binary_accuracy: 0.5995\n",
            "Epoch 15: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 7s 465ms/step - loss: 0.6740 - binary_accuracy: 0.5995 - val_loss: 0.6803 - val_binary_accuracy: 0.5822\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6739 - binary_accuracy: 0.5995\n",
            "Epoch 16: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 7s 466ms/step - loss: 0.6739 - binary_accuracy: 0.5995 - val_loss: 0.6798 - val_binary_accuracy: 0.5822\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6739 - binary_accuracy: 0.5995\n",
            "Epoch 17: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.6739 - binary_accuracy: 0.5995 - val_loss: 0.6803 - val_binary_accuracy: 0.5822\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6735 - binary_accuracy: 0.5995\n",
            "Epoch 18: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 485ms/step - loss: 0.6735 - binary_accuracy: 0.5995 - val_loss: 0.6796 - val_binary_accuracy: 0.5822\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6737 - binary_accuracy: 0.5995\n",
            "Epoch 19: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 493ms/step - loss: 0.6737 - binary_accuracy: 0.5995 - val_loss: 0.6797 - val_binary_accuracy: 0.5822\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6739 - binary_accuracy: 0.5995\n",
            "Epoch 20: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 524ms/step - loss: 0.6739 - binary_accuracy: 0.5995 - val_loss: 0.6803 - val_binary_accuracy: 0.5822\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6741 - binary_accuracy: 0.5995\n",
            "Epoch 21: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 524ms/step - loss: 0.6741 - binary_accuracy: 0.5995 - val_loss: 0.6797 - val_binary_accuracy: 0.5822\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6737 - binary_accuracy: 0.5995\n",
            "Epoch 22: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 509ms/step - loss: 0.6737 - binary_accuracy: 0.5995 - val_loss: 0.6809 - val_binary_accuracy: 0.5822\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6750 - binary_accuracy: 0.5995\n",
            "Epoch 23: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 486ms/step - loss: 0.6750 - binary_accuracy: 0.5995 - val_loss: 0.6799 - val_binary_accuracy: 0.5822\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6735 - binary_accuracy: 0.5995\n",
            "Epoch 24: loss did not improve from 0.67280\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.6735 - binary_accuracy: 0.5995 - val_loss: 0.6795 - val_binary_accuracy: 0.5822\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6725 - binary_accuracy: 0.5995\n",
            "Epoch 25: loss improved from 0.67280 to 0.67248, saving model to model1.h5\n",
            "16/16 [==============================] - 9s 546ms/step - loss: 0.6725 - binary_accuracy: 0.5995 - val_loss: 0.6800 - val_binary_accuracy: 0.5822\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x171e544f5b0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_n = ModelCheckpoint('model1.h5',verbose=1, monitor='loss',save_best_only=True, mode='auto')\n",
        "\n",
        "\n",
        "cnnModel.fit(x=X_train_seq_trunc_n, y=y_train, batch_size=256, epochs=25, \n",
        "                validation_data=(X_test_seq_trunc_n,y_test), callbacks=[checkpoint_n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnnModel = keras.models.load_model('model1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6800 - binary_accuracy: 0.5822\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6799591183662415, 0.5821643471717834]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = cnnModel.evaluate(X_test_seq_trunc_n, y_test)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cnnModel = Sequential()\n",
        "cnnModel.add(layers.Embedding(vocabSize, 16, input_length=1000))\n",
        "cnnModel.add(layers.Dropout(0.2))\n",
        "\n",
        "cnnModel.add(layers.Convolution1D(32,4,activation='relu'))\n",
        "cnnModel.add(layers.Dropout(0.3))\n",
        "\n",
        "cnnModel.add(layers.AveragePooling1D())\n",
        "\n",
        "cnnModel.add(layers.Convolution1D(64,4,activation='relu'))\n",
        "cnnModel.add(layers.Dropout(0.4))\n",
        "\n",
        "cnnModel.add(layers.AveragePooling1D())\n",
        "\n",
        "cnnModel.add(layers.Flatten())\n",
        "cnnModel.add(layers.Dropout(0.6))\n",
        "\n",
        "cnnModel.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "cnnModel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "standard = StandardScaler().fit(X_train_seq_trunc)\n",
        "X_train_seq_trunc_s = standard.transform(X_train_seq_trunc)\n",
        "X_test_seq_trunc_s = standard.transform(X_test_seq_trunc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.11383641, -0.11465849, -0.11532675, ...,  0.04941733,\n",
              "        -0.40346006,  1.06823431],\n",
              "       [10.55684767, -0.11182726, -0.11056632, ..., -0.17497167,\n",
              "        -0.41655438, -0.16237554],\n",
              "       [-0.11383641, -0.11465849, -0.11532675, ...,  0.07730582,\n",
              "        -0.23914757, -0.26955743],\n",
              "       ...,\n",
              "       [-0.11383641, -0.11465849, -0.11532675, ..., -0.39216683,\n",
              "        -0.3531948 , -0.35902397],\n",
              "       [ 0.38250842,  0.07715745, -0.11430666, ..., -0.21981005,\n",
              "         1.18538684, -0.40052156],\n",
              "       [-0.11383641, -0.11465849, -0.11532675, ..., -0.03848561,\n",
              "        -0.40293207, -0.36751763]])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_seq_trunc_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6778 - binary_accuracy: 0.5965\n",
            "Epoch 1: loss improved from inf to 0.67784, saving model to model2.h5\n",
            "16/16 [==============================] - 8s 433ms/step - loss: 0.6778 - binary_accuracy: 0.5965 - val_loss: 0.6809 - val_binary_accuracy: 0.5822\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6738 - binary_accuracy: 0.5995\n",
            "Epoch 2: loss improved from 0.67784 to 0.67380, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 0.6738 - binary_accuracy: 0.5995 - val_loss: 0.6798 - val_binary_accuracy: 0.5822\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6735 - binary_accuracy: 0.5995\n",
            "Epoch 3: loss improved from 0.67380 to 0.67353, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 412ms/step - loss: 0.6735 - binary_accuracy: 0.5995 - val_loss: 0.6796 - val_binary_accuracy: 0.5822\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6741 - binary_accuracy: 0.5995\n",
            "Epoch 4: loss did not improve from 0.67353\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.6741 - binary_accuracy: 0.5995 - val_loss: 0.6801 - val_binary_accuracy: 0.5822\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6736 - binary_accuracy: 0.5995\n",
            "Epoch 5: loss did not improve from 0.67353\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6736 - binary_accuracy: 0.5995 - val_loss: 0.6800 - val_binary_accuracy: 0.5822\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6728 - binary_accuracy: 0.5995\n",
            "Epoch 6: loss improved from 0.67353 to 0.67276, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.6728 - binary_accuracy: 0.5995 - val_loss: 0.6792 - val_binary_accuracy: 0.5822\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6726 - binary_accuracy: 0.5995\n",
            "Epoch 7: loss improved from 0.67276 to 0.67258, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.6726 - binary_accuracy: 0.5995 - val_loss: 0.6795 - val_binary_accuracy: 0.5822\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6710 - binary_accuracy: 0.5995\n",
            "Epoch 8: loss improved from 0.67258 to 0.67101, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6710 - binary_accuracy: 0.5995 - val_loss: 0.6788 - val_binary_accuracy: 0.5822\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6695 - binary_accuracy: 0.5995\n",
            "Epoch 9: loss improved from 0.67101 to 0.66946, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6695 - binary_accuracy: 0.5995 - val_loss: 0.6793 - val_binary_accuracy: 0.5822\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6674 - binary_accuracy: 0.6018\n",
            "Epoch 10: loss improved from 0.66946 to 0.66737, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 398ms/step - loss: 0.6674 - binary_accuracy: 0.6018 - val_loss: 0.6804 - val_binary_accuracy: 0.5802\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6628 - binary_accuracy: 0.6048\n",
            "Epoch 11: loss improved from 0.66737 to 0.66275, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.6628 - binary_accuracy: 0.6048 - val_loss: 0.6804 - val_binary_accuracy: 0.5782\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6624 - binary_accuracy: 0.6036\n",
            "Epoch 12: loss improved from 0.66275 to 0.66240, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.6624 - binary_accuracy: 0.6036 - val_loss: 0.6829 - val_binary_accuracy: 0.5792\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6596 - binary_accuracy: 0.6106\n",
            "Epoch 13: loss improved from 0.66240 to 0.65960, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.6596 - binary_accuracy: 0.6106 - val_loss: 0.6829 - val_binary_accuracy: 0.5792\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6535 - binary_accuracy: 0.6131\n",
            "Epoch 14: loss improved from 0.65960 to 0.65346, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 426ms/step - loss: 0.6535 - binary_accuracy: 0.6131 - val_loss: 0.6867 - val_binary_accuracy: 0.5802\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6514 - binary_accuracy: 0.6096\n",
            "Epoch 15: loss improved from 0.65346 to 0.65141, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.6514 - binary_accuracy: 0.6096 - val_loss: 0.6897 - val_binary_accuracy: 0.5812\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6485 - binary_accuracy: 0.6231\n",
            "Epoch 16: loss improved from 0.65141 to 0.64848, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 0.6485 - binary_accuracy: 0.6231 - val_loss: 0.6989 - val_binary_accuracy: 0.5822\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6453 - binary_accuracy: 0.6279\n",
            "Epoch 17: loss improved from 0.64848 to 0.64533, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 440ms/step - loss: 0.6453 - binary_accuracy: 0.6279 - val_loss: 0.6922 - val_binary_accuracy: 0.5752\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6437 - binary_accuracy: 0.6209\n",
            "Epoch 18: loss improved from 0.64533 to 0.64370, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 432ms/step - loss: 0.6437 - binary_accuracy: 0.6209 - val_loss: 0.6903 - val_binary_accuracy: 0.5741\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6421 - binary_accuracy: 0.6239\n",
            "Epoch 19: loss improved from 0.64370 to 0.64214, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 459ms/step - loss: 0.6421 - binary_accuracy: 0.6239 - val_loss: 0.6970 - val_binary_accuracy: 0.5711\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6393 - binary_accuracy: 0.6314\n",
            "Epoch 20: loss improved from 0.64214 to 0.63930, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 408ms/step - loss: 0.6393 - binary_accuracy: 0.6314 - val_loss: 0.6931 - val_binary_accuracy: 0.5762\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6306 - binary_accuracy: 0.6359\n",
            "Epoch 21: loss improved from 0.63930 to 0.63060, saving model to model2.h5\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.6306 - binary_accuracy: 0.6359 - val_loss: 0.7007 - val_binary_accuracy: 0.5741\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6292 - binary_accuracy: 0.6394\n",
            "Epoch 22: loss improved from 0.63060 to 0.62917, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 422ms/step - loss: 0.6292 - binary_accuracy: 0.6394 - val_loss: 0.7098 - val_binary_accuracy: 0.5812\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6322 - binary_accuracy: 0.6339\n",
            "Epoch 23: loss did not improve from 0.62917\n",
            "16/16 [==============================] - 6s 407ms/step - loss: 0.6322 - binary_accuracy: 0.6339 - val_loss: 0.7042 - val_binary_accuracy: 0.5822\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6249 - binary_accuracy: 0.6444\n",
            "Epoch 24: loss improved from 0.62917 to 0.62493, saving model to model2.h5\n",
            "16/16 [==============================] - 7s 424ms/step - loss: 0.6249 - binary_accuracy: 0.6444 - val_loss: 0.7115 - val_binary_accuracy: 0.5802\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6300 - binary_accuracy: 0.6304\n",
            "Epoch 25: loss did not improve from 0.62493\n",
            "16/16 [==============================] - 7s 431ms/step - loss: 0.6300 - binary_accuracy: 0.6304 - val_loss: 0.7091 - val_binary_accuracy: 0.5731\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x171ebadf910>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_s = ModelCheckpoint('model2.h5',verbose=1, monitor='loss',save_best_only=True, mode='auto')\n",
        "\n",
        "\n",
        "cnnModel.fit(x=X_train_seq_trunc_s, y=y_train, batch_size=256, epochs=25, \n",
        "                validation_data=(X_test_seq_trunc_s,y_test), callbacks=[checkpoint_s])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnnModel = keras.models.load_model('model2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7115 - binary_accuracy: 0.5802\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7115397453308105, 0.5801603198051453]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = cnnModel.evaluate(X_test_seq_trunc_s, y_test)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "D:\\Imp Programs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5781563126252505"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "logi = LogisticRegressionCV(Cs=20, cv=3, random_state=42, max_iter=100)\n",
        "logi.fit(X_train_seq_trunc, y_train)\n",
        "\n",
        "y_pred = logi.predict(X_test_seq_trunc)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "caseStudy.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3b97bdc2c87e206e8291e5ab4f714e0f91846bfc9349b2ee1278ba67b5e1fb2e"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
